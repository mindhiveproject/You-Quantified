[
  {
    "device": "EMOTIV",
    "short_description": "A consumer-grade electroencephalography (EEG) headset for research, cognitive training, and brain-computer interfaces (BCIs). EMOTIV captures raw EEG signals, derives metrics, and tracks head motion.",
    "description": "The EMOTIV headset is a consumer-grade EEG device designed for neuroscience research, cognitive training, and brain-computer interface (BCI) applications. Equipped with multiple sensors, it captures high-resolution brain activity data across various regions of the scalp.",
    "image": "/EMOTIV_EPOC.webp",
    "type": "EEG",
    "connections": "multiple",
    "sampling_rate": 128,
    "data": [
      {
        "name": "Theta",
        "description": "Band power from (4-8Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Alpha",
        "description": "Band power from (8-12Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Low beta",
        "description": "Band power from (12-16Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "High beta",
        "description": "Band power from (16-25Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Gamma",
        "description": "Band power from (25-45Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "X",
        "description": "X-axis gyroscope data for the EMOTIV headset",
        "type": "Movement"
      },
      {
        "name": "Y",
        "description": "Y-axis gyroscope data for the EMOTIV headset",
        "type": "Movement"
      },
      {
        "name": "Z",
        "description": "Z-axis gyroscope data for the EMOTIV headset",
        "type": "Movement"
      },
      {
        "name": "AF3",
        "description": "AF3 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "F7",
        "description": "F7 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "F3",
        "description": "F3 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "FC5",
        "description": "FC5 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "T7",
        "description": "T7 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "P7",
        "description": "P7 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "O1",
        "description": "O1 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "O2",
        "description": "O2 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "P8",
        "description": "P8 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "T8",
        "description": "T8 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "FC6",
        "description": "FC6 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "F4",
        "description": "F4 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "F8",
        "description": "F8 channel of raw EEG",
        "type": "Raw EEG channels"
      },
      {
        "name": "AF4",
        "description": "AF4 channel of raw EEG",
        "type": "Raw EEG channels"
      }
    ]
  },
  {
    "device": "Muse",
    "description": "A wearable electroencephalography (EEG) headset that tracks brain activity, offering real-time insights into focus and relaxation.",
    "short_description": "The Muse is a popular device for meditation, stress management, and neurofeedback training. It captures and streams brain signals from the frontal and temporal lobes, while also providing heart rate data through its integrated photoplethysmography (PPG) sensors.",
    "image": "/Muse2.webp",
    "type": "EEG & PPG",
    "connections": "multiple",
    "sampling_rate": {
      "EEG": 256,
      "PPG": 64,
      "Band Powers": 10
    },
    "data": [
      {
        "name": "Theta",
        "description": "Band power from (4-8Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Alpha",
        "description": "Band power from (8-12Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Low beta",
        "description": "Band power from (12-16Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "High beta",
        "description": "Band power from (16-25Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "Gamma",
        "description": "Band power from (25-45Hz)",
        "type": "EEG Metrics"
      },
      {
        "name": "TP9",
        "description": "EEG activity from the left temporal lobe.",
        "type": "Raw EEG channels"
      },
      {
        "name": "TP10",
        "description": "EEG activity from the right temporal lobe.",
        "type": "Raw EEG channels"
      },
      {
        "name": "AF7",
        "description": "EEG activity from the left frontal lobe.",
        "type": "Raw EEG channels"
      },
      {
        "name": "AF8",
        "description": "EEG activity from the right temporal lobe.",
        "type": "Raw EEG channels"
      },
      {
        "name": "PPG",
        "description": "Photoplethysmography activity from the forehead.",
        "type": "PPG"
      },
      {
        "name": "Heart rate",
        "description": "Heart rate derived from PPG.",
        "type": "Heart rate"
      }
    ]
  },
  {
    "device": "Face Landmarker",
    "short_description": "Streams face movements and expressions from a live video feed.",
    "description": "This device uses machine learning models to analyze a live video feed and generate scores representing facial expressions and movements. To ensure proper functionality, keep the window active in the background during operation.",
    "image": "/face_landmark.png",
    "type": "Video",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Facial Movements",
        "description": "Different Facial Movements",
        "type": "Face Landmarks"
      }
    ]
  },
  {
    "device": "Pose Detection",
    "short_description": "This device allows you to use your webcam to track the movements of your body.",
    "description": "This device allows you to use your webcam to track the movements of your body.",
    "image": "/face_landmark.png",
    "type": "Video",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Body Movements",
        "description": "Different limb positions",
        "type": "Pose Landmarks"
      }
    ]
  },
  {
    "device": "Face Synchrony",
    "short_description": "Measure synchrony in facial expressions during live video tracking.",
    "description": "This device tracks facial landmarks from a live video feed to measure the synchrony between facial expressions. Values closer to one indicate greater similarity in facial landmark positions, while lower values reflect reduced synchrony.",
    "image": "/face_landmark.png",
    "type": "Video",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Face Synchrony",
        "description": "Synchrony between facial expressions",
        "type": "Synchrony & Face Landmarks"
      }
    ]
  },
  {
    "device": "Video Heart Rate",
    "short_description": "This demo runs a simple variant of rPPG directly in your browser to measure your heart rate based on subtle changes in skin color.",
    "description": "This demo runs a simple variant of rPPG directly in your browser to measure your heart rate based on subtle changes in skin color captured by your webcam. For best results, try in a constantly well lit space with minimal device and subject motion.",
    "image": "/HeartRateLine.png",
    "type": "Video",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Heart rate",
        "description": "Heart rate estimated from changes in the color of your face",
        "type": "Heart rate"
      }
    ]
  },
  {
    "device": "Audio Volume",
    "short_description": "Measure your microphoneâ€™s audio volume with a single metric.",
    "description": "This tool calculates microphone audio volume using the Root Mean Square (RMS) method. Levels are expressed in decibels relative to full scale (dBFS), with values represented as negative numbers relative to the maximum peak (0 dBFS).",
    "image": "/speaker-high-volume.png",
    "type": "Audio",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Volume",
        "description": "The Root Mean Square of the values of the signal",
        "type": "Audio Feature"
      }
    ]
  },
  {
    "device": "Voice Emotion",
    "short_description": "Analyze the valence and arousal of emotions in your voice.",
    "description": "This tool uses Mel-Frequency Cepstral Coefficients (MFCCs) to analyze voice emotion by estimating valence (positivity or negativity) and arousal (energy level). It provides insights into emotional expression from audio input.",
    "type": "Audio",
    "connections": "single",
    "sampling_rate": 15,
    "data": [
      {
        "name": "Voice Emotion",
        "description": "The valence and arousal of emotion in your voice",
        "type": "Audio Feature"
      }
    ]
  },
  {
    "device": "Video Emotion",
    "short_description": "Calculate the probability of different emotions from a live video feed.",
    "description": "This device uses a pre-trained deep learning model to estimate the probability of different emotions (e.g., happy, sad, angry) from a live video feed. The model processes facial expressions to predict emotional states.",
    "type": "Video",
    "connections": "single",
    "sampling_rate": 24,
    "data": [
      {
        "name": "Anger",
        "description": "Probability of anger in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Disgust",
        "description": "Probability of disgust in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Fear",
        "description": "Probability of fear in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Happy",
        "description": "Probability of happiness in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Sad",
        "description": "Probability of sadness in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Surprise",
        "description": "Probability of surprise in the video",
        "type": "Emotion Feature"
      },
      {
        "name": "Neutral",
        "description": "Probability of neutrality in the video",
        "type": "Emotion Feature"
      }
    ]
  }
]